---
title: "Efficient Exploration for Embodied Question Answering"
authors_before: "[Allen Ren](https://allenzren.github.io/)"
authors_after: ", [Anushri Dixit](https://www.anushridixit.com/), [Masha Itkina](https://mashaitkina.weebly.com/), [Anirudha Majumdar](https://mae.princeton.edu/people/faculty/majumdar), [Dorsa Sadigh](https://dorsa.fyi/)"
award: ""
collection: publications
permalink: /publication/dib
tldr: 'We combine VLM semantic reasoning and rigorous uncertainty quantification to enable agents to efficiently explore relevant regions of unknown 3D environments, and stop to answer questions about them with calibrated confidence.'
date: 2024-03-01
venue: 'RSS 2024'
preprint: ''
header: 
  teaser: 'explore.gif'
paper: 'https://arxiv.org/abs/2403.15941'
code: 'https://github.com/Stanford-ILIAD/explore-eqa' 
link: 'https://explore-eqa.github.io'
video: ''
twitter: 'https://x.com/allenzren/status/1773453887816798604'
categories:
  - Robot learning
  - Foundation models
---
